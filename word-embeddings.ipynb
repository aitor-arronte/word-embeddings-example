{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "887db6fa518940158f28dcf9ee83528b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 13:56:06 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-06-13 13:56:07 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-06-13 13:56:07 INFO: Use device: cpu\n",
      "2022-06-13 13:56:07 INFO: Loading: tokenize\n",
      "2022-06-13 13:56:07 INFO: Loading: pos\n",
      "2022-06-13 13:56:07 INFO: Loading: lemma\n",
      "2022-06-13 13:56:07 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NLTK result: ['once', 'yawn', 'slowly', 'get', 'foot', 'shake']\n",
      " Stanza result: ['once', 'yawn', 'slowly', 'get', 'foot', 'shake']\n"
     ]
    }
   ],
   "source": [
    "#FIRST: Install requirements.txt\n",
    "#we first import Gensim library to use word2vec, and also NLTK to tokenize (could be done with any other library)\n",
    "#See https://radimrehurek.com/gensim/models/word2vec.html for API details\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import NLTKWordTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "#Input is a list containing lists of sentences.\n",
    "#See from gensim.test.utils import common_texts for a sample text\n",
    "\n",
    "#We will work with the Gutenberg datasets from NLTK as an example\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "txt = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
    "#Tokenize sentences first.\n",
    "sentences = sent_tokenize(txt)\n",
    "#We remove the title\n",
    "sentences = sentences[1:]\n",
    "\n",
    "#Normalize the input text by removing stopwords\n",
    "word_tokenizer = NLTKWordTokenizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "normalized_sentences = []\n",
    "normalized_stanza = []\n",
    "#We compare NLTK with Stanza to see differences and add POS to lemma.\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "for txt in sentences:\n",
    "    tkns = word_tokenizer.tokenize(txt)\n",
    "    tkns = [''.join(t.split('-')).lower() for t in tkns if\n",
    "\t\t        t not in stop_words and t not in '@.,!#$%*:;\"' and len(t)>2]\n",
    "    sent = nlp(' '.join(tkns))\n",
    "    sent = [s.lemma for s in sent.iter_words()]\n",
    "    normalized_sentences.append(sent)\n",
    "    sentence = nlp(txt)\n",
    "    lemmas = [w.lemma for w in sentence.iter_words() if w.text not in stop_words and w.text not in '@.,!#$%*:;\"' and len(w.text)>2]\n",
    "    normalized_stanza.append(lemmas)\n",
    "#Print a single sentence to see the result of the word tokenization, normalization, and lemmatization from both libraries\n",
    "print(f\" NLTK result: {normalized_sentences[0]}\")\n",
    "print(f\" Stanza result: {normalized_stanza[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Buster', 0.9837964177131653),\n ('one', 0.9802712202072144),\n ('little', 0.9796777367591858),\n ('could', 0.9788732528686523),\n ('Farmer', 0.9780943393707275),\n ('know', 0.9780401587486267),\n ('Brown', 0.97797691822052),\n ('berry', 0.9773219227790833),\n ('see', 0.9767408967018127),\n ('Green', 0.9766647219657898)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2vec parameterization is simple. Just provide list of lists of the sentences, the dimension of the output vector (100), window for the skip-gram\n",
    "# and the minimum count for words in the corpus (sentences list).\n",
    "model = Word2Vec(sentences=normalized_stanza, vector_size=100, window=3, min_count=3)\n",
    "model.wv.similar_by_word('boy', 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}